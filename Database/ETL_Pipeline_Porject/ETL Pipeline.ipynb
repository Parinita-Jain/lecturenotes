{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "<center><h1> Project: ETL Pipeline </h1></center>\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Importing the required libraries`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ETl---Lets say we have a restaurant and it has multiple sources that is generating data everyday like dbms,json,csv,xml\n",
    "\n",
    "# and it make predictions regarding future sales or some business decision are taken.\n",
    "\n",
    "# now the inputs are in different formats but for a decision making process we require that the data is in single format\n",
    "# like in a spreadsheet or in a database,so to bridge this gap between our raw data and clean data,we have etl process.\n",
    "# etl stands for extraction,transformation and load,in simple terms it prepares raw data for decision making\n",
    "\n",
    "# extract extracts data from various sources,this extraction can either be full or partial\n",
    "# transforming data involves preprocessing of data like removing null values,duplicate values and all of this happens in a\n",
    "# staging area..which is a temporary place like database where all of these transformation happens\n",
    "# load is the final step where we load data into target location..this loading can be of2 types, like incremental or full\n",
    "# when the data is loaded after extraction and transformation for the very first time, that is a full load\n",
    "# and any subsequent loading of data,for a new entry is known as a partial load.\n",
    "\n",
    "# the mechanism trough which etl process occur is called as etl pipeline,wecan automate etl pipeline and schedule it to run\n",
    "# based on business requirement\n",
    "\n",
    "# there are multiple rulesthat fecilitate using etl pipelines like,informatica,talend,pentaho,etc.\n",
    "\n",
    "# undersatnding the context--letssay there is an e-commerce website on which there are 100 differrent products to purchase\n",
    "# in 3 different categories- which are Ac, refrigerators,and microwave,\n",
    "# the data is availablein csv files and mysql tables\n",
    "\n",
    "#product data is stored in csv file,the data change in tis file rarely.New products are addedonce in months.\n",
    "# this file has 4 different colproduct id,productname,product category,product price\n",
    "# next we have user table. whenever we have new user the data is stored in this table.\n",
    "# in our website every 2 secs a new user is signing up and their details have been stored\n",
    "# in this tablewe have unique user id,user email,user name,source from which user came like instagram,fb,etc,isprime,signuptime\n",
    "# transaction table-if any product is soldin our website,then the trx details arestored here-on an avg every5-15 secs\n",
    "# a trxhappens in our website,and wehave 5 different cols--transaction id,user id ,product id,transaction time,price\n",
    "# our website also has a page where people can apply for a refund\n",
    "# now there are some rules regarding refund--refund will be done only if the trx is done in last 48 hrs.\n",
    "# the details of refund is stored in refund table,wehave ticket id,user name,transaction id,transaction amount,ticket raise time\n",
    "# the company automatically do refunds in 30mis, so we need to validate the request before initiating refund.\n",
    "# so for that we will create a new table called valid refund table which has-ticket id, trx id,user id,price,ticket raise time\n",
    "\n",
    "# so we have 1 sql file and 4 mysql tables apart fromthat we have 1 more table called accounts table which has details of\n",
    "# account number of customers-- but for the scope of this project lets suppose we dont have access tothat.\n",
    "\n",
    "# understanding requirements of etl pipeline---\n",
    "# signup summary gets updated every 5 mins , and we need data added in the users table in the last 5 mins to generate the results\n",
    "# this is the first pipeline that we need to build.\n",
    "# another requirement is we need to generate a trx summary table-in this table we will store the no.of trxs product category wise \n",
    "# and product brand wise. It will also store the total sales brand wise and product wise.This table gets updated every 10 mins\n",
    "# to calc this data we will require trx tableand product data csv file.This is the second pipeline\n",
    "# another requirement is we need to update valid refund table-for that we require data from 3 different sources-\n",
    "# 1 refund table, we will collect last request every 30 mins, trxs- trxids generated in last 48 hours, from valid refunds\n",
    "# we will check if the data is refund or not in a last 48hrs.\n",
    "# so we have 2 types of reuests valid and invalid,we will put the result in csv file and we will assign the reason as well\n",
    "\n",
    "# invalid request csv has ticket id,user name,refund reject reason\n",
    "# apart fromthat we need to update the valid trx table for which the refund req is found correct\n",
    "\n",
    "# this is the 3rd etl pipeline, and it runs every 30 mins\n",
    "\n",
    "\n",
    "# now goto sql workbench---import etl_dump.sqlscript in workbench and create allthe tables--etl_dump.sql file\n",
    "\n",
    "# then see 1.Generate User Data.ipynb-- u donot need to understand this file fully\n",
    "\n",
    "\n",
    "# keep on running 1.Generate User Data.ipynb in the mean time goto workbench\n",
    "\n",
    "#select count(*) from users; 246 rows have been entered.\n",
    "\n",
    "# because generate user data.ipynb is running, rows will be kept on adding in users.u\n",
    "\n",
    "#then goto 2.generate transactions data\n",
    "\n",
    "# in product_table.csv we have product_id,product_name,product_categoryand product_price-- we are not updating anything\n",
    "\n",
    "# in  this file. Then we have 3. generate refund table-\n",
    "\n",
    "# now,above we have setup a simulation environment to generate realtime data.Now,before going into etl pipeline--we\n",
    "# will first undrstand our pipeline--\n",
    "# for extraction we have extract_users_data() will extract data oflast 5 mins fromusers table.-- this func will pass the \n",
    "# data to transform_users_data() wich will do required transformation .Finally,load_user_summary() for loading data into\n",
    "# target table.All these steps will be executed in order every 5mins.\n",
    "# for the 2nd pipeline ,we need to extract product data from csv file fo which define extract_product_data().. we also\n",
    "# need to extract trx data in extract_transaction_data()... data from both these functions are passed into transform_trx_data()\n",
    "# and finally load_transaction_summary() will load the data into required target table--- all these steps will be executed \n",
    "# every 10 mins\n",
    "# For the finalpipeline, we need to extract_transaction_data(),extract_refund_data(),extract_valid_refund_data()-- we\n",
    "# will use all these result and pass it into transform_refund_data() for required trnsformation and finally load_refund_table()\n",
    "# togenerate invalid request csv file and also upload vald data into refund table-- Nowthis pipeline is executed every 30 mins\n",
    "\n",
    "# for all these pipelines we will needa single DB connection\n",
    "\n",
    "# now datain extract_product_data() changes rarely and we donot need to extract it every 10 mins. So we will use this func\n",
    "# outside the pipelin\n",
    "\n",
    "# now we will start building etl pipeline --\n",
    "\n",
    "# etl pipeline 1 to update user summary table--\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: schedule in c:\\users\\itvedant\\anaconda3\\lib\\site-packages (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "import schedule\n",
    "import time\n",
    "import pandas as pd\n",
    "import mysql.connector as mysql\n",
    "from datetime import timedelta\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "***Define a function to create the connection with the MySQL database. `Configure the following cell as per your system settings.`***\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the database\n",
    "def create_connection():\n",
    "    db = mysql.connect(\n",
    "    host = \"localhost\",\n",
    "    user = \"root\",  ## Enter your user name here\n",
    "    #password = \"ABC@123\", ## Enter your password here\n",
    "    database = \"etl\", ## Enter the database name here\n",
    "    #auth_plugin = \"mysql_native_password\",\n",
    "    )\n",
    "\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now for the entirety of this project keep the generate user data,trx data,refund data files running until we r finished \n",
    "# with all the pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "<center><h1> ETL Pipeline 1 </h1></center>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "![](images/pipeline-1.png)\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### `Define the Extraction Function`\n",
    "\n",
    "---\n",
    "\n",
    "* ***`extract_users_data()`***: It will extract the data from the `users` table within defined time range and return the dataframe.\n",
    "\n",
    "<br>\n",
    "\n",
    "* ***`Paramaters Required`***:\n",
    "    * `Database Connection String` is required so that it can connect to the database and query the data. \n",
    "    * `Start time` and `End time` is required so that we can modify the query.\n",
    "\n",
    "<br>\n",
    "\n",
    "* ***`Output`***: It will return the pandas dataframe of the extracted data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_users_data(db, start_time, end_time):\n",
    "    \n",
    "    # create database cursor\n",
    "    cursor = db.cursor()\n",
    "    \n",
    "    print(\"Extracting signup results between {} and {}\".format(str(start_time),str(end_time)))\n",
    "    \n",
    "    # command to extract the data of last 5 minutes.\n",
    "    command = \"SELECT * FROM users WHERE signup_time BETWEEN '{}' AND '{}'\".format(start_time, end_time)\n",
    "    \n",
    "    # execute the command and return the results.\n",
    "    cursor.execute(command)\n",
    "    data = cursor.fetchall()\n",
    "    \n",
    "    # return the dataframe\n",
    "    return pd.DataFrame.from_records(data, columns= ['user_id',\n",
    "                                                     'user_email',\n",
    "                                                     'user_name',\n",
    "                                                     'source',\n",
    "                                                     'is_prime',\n",
    "                                                     'signup_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Define the Trasformation Function`\n",
    "\n",
    "---\n",
    "\n",
    "* ***`transform_users_data()`***: It will use the data from the `extract_users_data()` of the last 5 minutes and do the following transformation using pandas.\n",
    "    * Replace the category `Not Available` with the `Organic` in the source column.\n",
    "    * Use the groupby function to calculate number of users in each category of source.\n",
    "    * Use the groupby function to calculate number of prime users in each category of source.\n",
    "    * Store all the results in a dictionary \n",
    "    * Add the start & end time in the dictionary and return it.\n",
    "    \n",
    "<br>    \n",
    "\n",
    "* ***`Paramaters Required`***:\n",
    "    * `df_user` user data dataframe of last 5 minutes. \n",
    "    * `Start time` and `End time` is required to update the output, so that we know the signup summary is between this particulae time range.\n",
    "\n",
    "<br>\n",
    "\n",
    "* ***`Output`***: It will return the dictionary that will be used to import the data into the transaction summary table.\n",
    "\n",
    "---\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the tranform function\n",
    "def transform_user_data(df_user, start_time, end_time):\n",
    "    \n",
    "    print(\"Transforming User Data...\")\n",
    "    \n",
    "    # replace the \"Not Available\" with the \"Organic\"\n",
    "    df_user.source.replace(\"Not Available\", \"Organic\", inplace=True)\n",
    "    \n",
    "    # groupby to calculate the number of users in each category of source.\n",
    "    source_count = df_user.groupby(['source'])['user_id'].count()\n",
    "    \n",
    "    # groupby to calculate the number of prime users in each category of source.\n",
    "    prime_count  = df_user.groupby(['source'])['is_prime'].sum()\n",
    "    \n",
    "    # create dictionary of source count\n",
    "    source_count_dict = source_count.to_dict()\n",
    "    \n",
    "    # append prime count in the same dictionary\n",
    "    for key in prime_count.to_dict():\n",
    "        new_key_name = \"prime_from_\" + key\n",
    "        source_count_dict[new_key_name] = prime_count[key]\n",
    "    \n",
    "    # add start_time and end_time to the dictionary \n",
    "    source_count_dict['start_time'] = str(start_time)\n",
    "    source_count_dict['end_time'] = str(end_time)\n",
    "    \n",
    "    # return the final dictionary\n",
    "    return source_count_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Define the Loading Function`\n",
    "\n",
    "---\n",
    "\n",
    "* ***`load_users_summary()`***: It will use the results dictionary from the `transform_users_data` and load it into the `signup_summary_table`.\n",
    "\n",
    "    \n",
    "<br>    \n",
    "\n",
    "* ***`Paramaters Required`***:\n",
    "\n",
    "    * `result_dict` final results dictionary from the transform function. \n",
    "    * `db` database connection string to update the values in the table.\n",
    "\n",
    "<br>\n",
    "\n",
    "* ***`Output`***: It will not return anything.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_user_summary(result_dict, db):\n",
    "    print(\"Loading User Summary Table...\")\n",
    "    cursor = db.cursor()\n",
    "    \n",
    "    # command to insert the data into the signup summary table using result dict\n",
    "    command = \"INSERT INTO signup_summary({col}) values{val}\".format(col= ','.join(result_dict.keys()),\n",
    "                                                                     val= tuple(result_dict.values()))\n",
    "    cursor.execute(command)\n",
    "    db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "#### `Define the ETL Pipeline`\n",
    "\n",
    "<br>\n",
    "\n",
    "Now, we will define the pipeline function, we will take the database connection as the parameter and do the following steps.\n",
    "\n",
    " * Now, we will define the time for which we want to extract the data.\n",
    " * Extract the latest 5 minutes of users data.\n",
    " * Transform it to get the users signup summary.\n",
    " * Load the data into the signup_summary table.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def pipeline_to_update_user_summary(db_object):\n",
    "    \n",
    "    # get the current time and time before 5 minutes\n",
    "    current_time = datetime.datetime.now()\n",
    "    current_minus_5 = current_time - datetime.timedelta(minutes=5)\n",
    "    \n",
    "    print(\"========================================================================================\")\n",
    "    print(\"Starting ETL to update user summary!!\")\n",
    "    \n",
    "    ## EXTRACTION\n",
    "    latest_user_data = extract_users_data(db = db_object,\n",
    "                                          start_time = current_minus_5,\n",
    "                                          end_time=current_time)\n",
    "    \n",
    "    ## TRANSFORMATION\n",
    "    user_summary_data = transform_user_data(df_user= latest_user_data,\n",
    "                                            start_time=current_minus_5, \n",
    "                                            end_time=current_time)\n",
    "    \n",
    "    ## LOADING\n",
    "    load_user_summary(result_dict = user_summary_data,\n",
    "                      db = db_object)\n",
    "    \n",
    "    print(\"Successfully loaded the data into user summary table !!\")\n",
    "    print(\"========================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "<center><h1> ETL Pipeline 2 </h1></center>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "![](images/pipeline-2.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Define the extraction functions`\n",
    "\n",
    "---\n",
    "\n",
    "* ***`extract_products_data()`***: It will read the products data from the CSV file. \n",
    "\n",
    "* ***`Paramaters Required`***: CSV file path is required. It is present in the `dataset` folder.\n",
    "\n",
    "* ***`Output`***: It will return the pandas dataframe of the CSV file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to extract the product_data from the CSV file\n",
    "def extract_products_data(file_path):\n",
    "    return pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "* ***`extract_transaction_data()`***: It will extract the data from the `transaction` table within a defined time range and return the dataframe.\n",
    "\n",
    "* ***`Paramaters Required`***:\n",
    "    * `Database Connection String` is required so that it can connect to the database and query the data. \n",
    "    * `Start time` and `End time` is required so that we can modify the query.\n",
    "\n",
    "* ***`Output`***: It will return the pandas dataframe of the extracted data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_transaction_data(db, start_time, end_time):\n",
    "    \n",
    "    # create database cursor\n",
    "    cursor = db.cursor()\n",
    "    \n",
    "    print(\"Extracting transactions between {} and {}\".format(str(start_time),str(end_time)))\n",
    "    \n",
    "    # command to extract the data of last 5 minutes.\n",
    "    command = \"SELECT * FROM transaction WHERE transaction_time BETWEEN '{}' AND '{}'\".format(start_time, end_time)\n",
    "    \n",
    "    # execute the command and return the results.\n",
    "    cursor.execute(command)\n",
    "    data = cursor.fetchall()\n",
    "    \n",
    "    # return the dataframe\n",
    "    return pd.DataFrame.from_records(data, columns= ['transaction_id',\n",
    "                                                     'user_id',\n",
    "                                                     'product_id',\n",
    "                                                     'transaction_time',\n",
    "                                                     'price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Define the Trasformation Function`\n",
    "\n",
    "---\n",
    "\n",
    "* ***`transform_transaction_data()`***: It will use the data from the `extract_transform_data()` of the last 10 minutes and the output of `extract_products_data()` and do the following transformation using pandas.\n",
    "    * Do the left join on `transction_data` and the `products_data`.\n",
    "    * Split the product_name and create a new feature `brand`.\n",
    "    * Use groupby to calculate the brand-wise sales.\n",
    "    * Use groupby to calculate the category-wise sales. \n",
    "    * Create dictionary of the calculated results.\n",
    "    * Add start and end time to the dictionary.\n",
    "    \n",
    "<br>    \n",
    "\n",
    "* ***`Paramaters Required`***:\n",
    "    * `df_transaction` transaction data dataframe of last 10 minutes. \n",
    "    * `df_product` product data frame.\n",
    "    * `Start time` and `End time` is required to update the output, so that we know the signup summary is between this particulae time range.\n",
    "\n",
    "<br>\n",
    "\n",
    "* ***`Output`***: It will return the dictionary which will be used to import the data into transaction summary.\n",
    "\n",
    "---\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the tranform function\n",
    "def transform_transaction_data(df_transaction, df_product, start_time, end_time):\n",
    "    \n",
    "    print(\"Transforming Transaction Data...\")\n",
    "    \n",
    "    # merge the transaction and product dataframe.\n",
    "    merged_df = df_transaction.merge(df_product, how='left', on='product_id')\n",
    "    \n",
    "    # split the product_name to get the brand  \n",
    "    merged_df['brand'] = merged_df['product_name'].apply(lambda x: x.split()[0])\n",
    "    \n",
    "    # calculate the brand count\n",
    "    brand_count = merged_df.groupby(['brand'])['transaction_id'].count()\n",
    "    \n",
    "    # calculate the category count\n",
    "    category_count = merged_df.groupby(['product_category'])['transaction_id'].count()\n",
    "    \n",
    "    # calculate the brand wise sales\n",
    "    brand_wise_sales    = merged_df.groupby(['brand'])['price'].sum()\n",
    "    \n",
    "    # calculate the category wise sales\n",
    "    category_wise_sales = merged_df.groupby(['product_category'])['price'].sum()\n",
    "    \n",
    "    # create dictionary \n",
    "    brand_count_dict = brand_count.to_dict()\n",
    "    \n",
    "    # append brand_count to dictionary\n",
    "    for key in category_count.to_dict():\n",
    "        brand_count_dict[key] = category_count[key]\n",
    "    \n",
    "    # append brand wise sales to dictionary\n",
    "    for key in brand_wise_sales.to_dict():\n",
    "        new_key_name = \"sales_from_\" + key\n",
    "        brand_count_dict[new_key_name] = brand_wise_sales[key]\n",
    "    \n",
    "    # append category wise sales to dictionary\n",
    "    for key in category_wise_sales.to_dict():\n",
    "        new_key_name = \"sales_from_\" + key\n",
    "        brand_count_dict[new_key_name] = category_wise_sales[key]\n",
    "    \n",
    "    # append start and end time to the dictionary\n",
    "    brand_count_dict['start_time'] = str(start_time)\n",
    "    brand_count_dict['end_time'] = str(end_time)\n",
    "    \n",
    "    return brand_count_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Define the Loading Function`\n",
    "\n",
    "---\n",
    "\n",
    "* ***`load_transaction_summary()`***: It will use the results dictionary from the `transform_transaction_data` and load it into the `transaction_summary_table`.\n",
    "\n",
    "    \n",
    "<br>    \n",
    "\n",
    "* ***`Paramaters Required`***:\n",
    "    * `result_dict` final results dictionary from the transform function. \n",
    "    * `db` database connection string to update the values in the table.\n",
    "\n",
    "<br>\n",
    "\n",
    "* ***`Output`***: It will not return anything.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transaction_summary(result_dict, db):\n",
    "    print(\"Loading Transaction Summary Table...\")\n",
    "    cursor = db.cursor()\n",
    "    \n",
    "    # command to insert the data into the transaction summary using the \n",
    "    command = \"INSERT INTO transaction_summary({col}) values{val}\".format(col= ', '.join(result_dict.keys()),\n",
    "                                                                       val= tuple(result_dict.values()))\n",
    "    \n",
    "    \n",
    "\n",
    "    command = command.replace(' Air Conditioner', '`Air Conditioner`')\n",
    "    command = command.replace('sales_from_Air Conditioner', '`sales_from_Air Conditioner`')\n",
    "    \n",
    "    cursor.execute(command)\n",
    "    db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_to_update_transaction_summary(db_object, products_data):\n",
    "    \n",
    "    current_time = datetime.datetime.now()\n",
    "    current_minus_10 = current_time - datetime.timedelta(minutes=10)\n",
    "    \n",
    "    print(\"========================================================================================\")\n",
    "    print(\"Starting ETL to update transaction summary!!\")\n",
    "    \n",
    "    ## EXTRACTION\n",
    "    latest_transaction_data = extract_transaction_data(db = db_object,\n",
    "                                                       start_time = current_minus_10,\n",
    "                                                       end_time = current_time)\n",
    "    \n",
    "    ## TRANSFORMATION\n",
    "    transaction_summary_data = transform_transaction_data(df_product = products_data,\n",
    "                                                          df_transaction = latest_transaction_data,\n",
    "                                                          start_time = current_minus_10,\n",
    "                                                          end_time = current_time)\n",
    "    ## LOADING\n",
    "    load_transaction_summary(result_dict = transaction_summary_data,\n",
    "                             db = db_object)\n",
    "    \n",
    "    print(\"Successfully loaded the data into transaction summary table !!\")\n",
    "    print(\"========================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "\n",
    "<center><h1> ETL Pipeline 3 </h1></center>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "![](images/pipeline-3.png)\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "* ***`extract_refund_data()`***: It will extract the data from the `refund_detail` table within the defined time range and return the dataframe.\n",
    "\n",
    "* ***`Paramaters Required`***:\n",
    "    * `Database Connection String` is required so that it can connect to the database and query the data. \n",
    "    * `Start time` and `End time` is required so that we can modify the query.\n",
    "\n",
    "* ***`Output`***: It will return the pandas dataframe of the CSV file.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_refund_data(db, start_time, end_time):\n",
    "    \n",
    "    # create database cursor\n",
    "    cursor = db.cursor()\n",
    "    \n",
    "    print(\"Extracting refund between {} and {}\".format(str(start_time),str(end_time)))\n",
    "    \n",
    "    # command to extract the data of last 30 minutes.\n",
    "    command = \"SELECT * FROM refund_detail WHERE ticket_raise_time BETWEEN '{}' AND '{}'\".format(start_time, end_time)\n",
    "    \n",
    "    # execute the command and return the results.\n",
    "    cursor.execute(command)\n",
    "    data = cursor.fetchall()\n",
    "    \n",
    "    # return the dataframe\n",
    "    return pd.DataFrame.from_records(data, columns= ['ticket_id',\n",
    "                                                     'user_name',\n",
    "                                                     'transaction_id',\n",
    "                                                     'transaction_amount',\n",
    "                                                     'ticket_raise_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "* ***`extract_valid_refund_data()`***: It will extract only the `transaction_id` from the `valid_refund` table within defined time range and return the dataframe.\n",
    "\n",
    "* ***`Paramaters Required`***:\n",
    "    * `Database Connection String` is required so that it can connect to the database and query the data. \n",
    "    * `Start time` and `End time` is required so that we can modify the query.\n",
    "\n",
    "* ***`Output`***: It will return the pandas dataframe of the CSV file.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_valid_refund_data(db, start_time, end_time):\n",
    "    \n",
    "    # create database cursor\n",
    "    cursor = db.cursor()\n",
    "    \n",
    "    print(\"Extracting valid refund data between {} and {}\".format(str(start_time),str(end_time)))\n",
    "    \n",
    "    # command to extract the data of last 30 minutes.\n",
    "    command = \"SELECT transaction_id FROM valid_refund WHERE ticket_raise_time BETWEEN '{}' AND '{}'\".format(start_time, end_time)\n",
    "    \n",
    "    # execute the command and return the results.\n",
    "    cursor.execute(command)\n",
    "    data = cursor.fetchall()\n",
    "    \n",
    "    # return the valid transaction IDs list\n",
    "    return pd.DataFrame.from_records(data, columns= ['ticket_raise_time'])['ticket_raise_time'].to_list()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Define the Trasformation Function`\n",
    "\n",
    "---\n",
    "\n",
    "* ***`transform_users_data()`***: It will use the data from the `extract_users_data()` of the last 5 minutes and do the following transformation using pandas.\n",
    "    * Replace the category `Not Available` with the `Organic` in the source column.\n",
    "    * Use the groupby function to calculate number of users in each category of source.\n",
    "    * Use the groupby function to calculate number of prime users in each category of source.\n",
    "    * Store all the results in a dictionary \n",
    "    * Add the start & end time in the dictionary and return it.\n",
    "    \n",
    "<br>    \n",
    "\n",
    "* ***`Paramaters Required`***:\n",
    "    * `df_user` user data dataframe of last 5 minutes. \n",
    "    * `Start time` and `End time` is required to update the output, so that we know the signup summary is between this particulae time range.\n",
    "\n",
    "<br>\n",
    "\n",
    "* ***`Output`***: It will return the dictionary with the user summary data.\n",
    "\n",
    "---\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_refund_data(df_refund, df_transactions, refund_issued_TID):\n",
    "    \n",
    "    print(\"Transforming Refund Data...\")\n",
    "    \n",
    "    valid_transactions = df_transactions.merge(df_refund, how='left', on='transaction_id')\n",
    "    valid_transactions = valid_transactions[valid_transactions.ticket_id.isnull() == False]\n",
    "    \n",
    "    \n",
    "    valid_transactions = valid_transactions.groupby(['transaction_id']).first().reset_index()\n",
    "    \n",
    "    valid_transactions_final = valid_transactions[~valid_transactions.transaction_id.isin(refund_issued_TID)]\n",
    "    valid_transaction_duplicate = valid_transactions[valid_transactions.transaction_id.isin(refund_issued_TID)]\n",
    "    \n",
    "    \n",
    "    \n",
    "    valid_transactions_final = valid_transactions_final[['ticket_id', 'transaction_id', 'user_id', 'price', 'ticket_raise_time']]\n",
    "    \n",
    "    valid_transactions_final = [tuple((row[0],row[1],row[2],int(row[3]), str(row[4]))) for row in valid_transactions_final.to_records(index=False)]\n",
    "    \n",
    "    \n",
    "    refund_rejected = df_refund.merge(df_transactions, how='left', on='transaction_id')\n",
    "    refund_rejected['refund_reject_reason'] = None\n",
    "    refund_rejected.loc[refund_rejected.product_id.isna() == True, 'refund_reject_reason'] = 'transaction_id_not_matched'\n",
    "    refund_rejected.loc[refund_rejected.transaction_id.isin(refund_issued_TID), 'refund_reject_reason'] = 'already_processed'\n",
    "    \n",
    "    \n",
    "    refund_rejected = refund_rejected[refund_rejected.refund_reject_reason.isna() == False]\n",
    "    refund_rejected = refund_rejected[['ticket_id', 'user_name', 'refund_reject_reason']]\n",
    "    \n",
    "    \n",
    "    return valid_transactions_final, refund_rejected\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Define the Loading Function`\n",
    "\n",
    "---\n",
    "\n",
    "* ***`load_users_summary()`***: It will use the results dictionary from the `transform_users_data` and load it into the `signup_summary_table`.\n",
    "\n",
    "    \n",
    "<br>    \n",
    "\n",
    "* ***`Paramaters Required`***:\n",
    "    * `result_dict` final results dictionary from the transform function. \n",
    "    * `db` database connection string to update the values in the table.\n",
    "\n",
    "<br>\n",
    "\n",
    "* ***`Output`***: It will not return anything.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_refund_data(valid_refunds, rejected_requests, db):\n",
    "    print(\"Loading Valid Refunds Table...\")\n",
    "    cursor = db.cursor()\n",
    "    \n",
    "    command = \"INSERT INTO valid_refund(ticket_id, transaction_id, user_id, price, ticket_raise_time) values(%s, %s, %s, %s, %s)\"\n",
    "    cursor.executemany(command, valid_refunds)\n",
    "    db.commit()\n",
    "    \n",
    "    file_name = \"rejected_request\" + str(pd.datetime.now()) + \".csv\"\n",
    "    rejected_requests.to_csv(\"output_folder/\" + file_name)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_to_update_valid_refunds(db_object):\n",
    "    \n",
    "    current_time = datetime.datetime.now()\n",
    "    current_minus_30 = current_time - datetime.timedelta(minutes = 30)\n",
    "    current_minus_48 = current_time - datetime.timedelta(hours  = 48)\n",
    "    \n",
    "    print(\"========================================================================================\")\n",
    "    print(\"Starting ETL to update valid refunds data!!\")\n",
    "    \n",
    "    ## EXTRACTION\n",
    "    latest_refund_requests = extract_refund_data(db = db_object,\n",
    "                                                 start_time = current_minus_30,\n",
    "                                                 end_time = current_time)\n",
    "    \n",
    "    valid_transactions = extract_transaction_data(db = db_object,\n",
    "                                                  start_time = current_minus_48,\n",
    "                                                  end_time = current_time)\n",
    "   \n",
    "    refund_issued = extract_valid_refund_data(db = db_object,\n",
    "                                              start_time = current_minus_48,\n",
    "                                              end_time = current_time)\n",
    "    \n",
    "    ## TRANSFORMATION\n",
    "    valid_refunds, refunds_rejected = transform_refund_data(df_refund=latest_refund_requests,\n",
    "                                                            df_transactions=valid_transactions,\n",
    "                                                            refund_issued_TID= refund_issued)\n",
    "    \n",
    "    ## LOADING\n",
    "    load_refund_data(db = db_object,\n",
    "                     valid_refunds = valid_refunds,\n",
    "                     rejected_requests = refunds_rejected)\n",
    "    \n",
    "    print(\"Successfully loaded the data into valid refund table !!\")\n",
    "    print(\"========================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<center><h1> Schedule Pipelines </h1></center>\n",
    "\n",
    "---\n",
    "\n",
    "![](images/pipeline-4.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Every 150 seconds do pipeline_to_update_valid_refunds(db_object=<mysql.connector.connection_cext.CMySQLConnection object at 0x0000021C05D2CA00>) (last run: [never], next run: 2023-01-18 09:56:01)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Schedule Pipelines\n",
    "\n",
    "db_object = create_connection()\n",
    "products_data = pd.read_csv('dataset/product_table.csv')\n",
    "\n",
    "schedule.every(30).seconds.do(pipeline_to_update_user_summary, db_object = db_object)\n",
    "schedule.every(60).seconds.do(pipeline_to_update_transaction_summary, db_object = db_object, products_data = products_data)\n",
    "schedule.every(150).seconds.do(pipeline_to_update_valid_refunds, db_object = db_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================\n",
      "Starting ETL to update user summary!!\n",
      "Extracting signup results between 2023-01-18 09:49:01.863751 and 2023-01-18 09:54:01.863751\n",
      "Transforming User Data...\n",
      "Loading User Summary Table...\n",
      "Successfully loaded the data into user summary table !!\n",
      "========================================================================================\n",
      "========================================================================================\n",
      "Starting ETL to update transaction summary!!\n",
      "Extracting transactions between 2023-01-18 09:44:31.784679 and 2023-01-18 09:54:31.784679\n",
      "Transforming Transaction Data...\n",
      "Loading Transaction Summary Table...\n",
      "Successfully loaded the data into transaction summary table !!\n",
      "========================================================================================\n",
      "========================================================================================\n",
      "Starting ETL to update user summary!!\n",
      "Extracting signup results between 2023-01-18 09:49:33.162888 and 2023-01-18 09:54:33.162888\n",
      "Transforming User Data...\n",
      "Loading User Summary Table...\n",
      "Successfully loaded the data into user summary table !!\n",
      "========================================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [45], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      2\u001b[0m         schedule\u001b[38;5;241m.\u001b[39mrun_pending() \n\u001b[1;32m----> 3\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "        schedule.run_pending() \n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create table valid_refund(ticket_id varchar(50), transaction_id varchar(50), user_id varchar(50), price int);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
