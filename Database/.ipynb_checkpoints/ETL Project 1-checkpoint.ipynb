{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "<center><h1> Project: Extract Transform Load.</h1></center>\n",
    "\n",
    "---\n",
    "\n",
    "----\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ETl---Lets say we have a restaurant and it has multiple sources that is generating data everyday like dbms,json,csv,xml\n",
    "\n",
    "# and it make predictions regarding future sales or some business decision are taken.\n",
    "\n",
    "# now the inputs are in different formats but for a decision making process we require that the data is in single format\n",
    "# like in a spreadsheet or in a database,so to bridge this gap between our raw data and clean data,we have etl process.\n",
    "# etl stands for extraction,transformation and load,in simple terms it prepares raw data for decision making\n",
    "\n",
    "# extract extracts data from various sources,this extraction can either be full or partial\n",
    "# transforming data involves preprocessing of data like removing null values,duplicate values and all of this happens in a\n",
    "# staging area..which is a temporary place like database where all of these transformation happens\n",
    "# load is the final step where we load data into target location..this loading can be of2 types, like incremental or full\n",
    "# when the data is loaded after extraction and transformation for the very first time, that is a full load\n",
    "# and any subsequent loading of data,for a new entry is known as a partial load.\n",
    "\n",
    "# the mechanism trough which etl process occur is called as etl pipeline,wecan automate etl pipeline and schedule it to run\n",
    "# based on business requirement\n",
    "\n",
    "# there are multiple rulesthat fecilitate using etl pipelines like,informatica,talend,pentaho,etc.\n",
    "\n",
    "# undersatnding the context--letssay there is an e-commerce website on which there are 100 differrent products to purchase\n",
    "# in 3 different categories- which are Ac, refrigerators,and microwave,\n",
    "# the data is availablein csv files and mysql tables\n",
    "\n",
    "#product data is stored in csv file,the data change in tis file rarely.New products are addedonce in months.\n",
    "# this file has 4 different colproduct id,productname,product category,product price\n",
    "# next we have user table. whenever we have new user the data is stored in this table.\n",
    "# in our website every 2 secs a new user is signing up and their details have been stored\n",
    "# in this tablewe have unique user id,user email,user name,source from which user came like instagram,fb,etc,isprime,signuptime\n",
    "# transaction table-if any product is soldin our website,then the trx details arestored here-on an avg every5-15 secs\n",
    "# a trxhappens in our website,and wehave 5 different cols--transaction id,user id ,product id,transaction time,price\n",
    "# our website also has a page where people can apply for a refund\n",
    "# now there are some rules regarding refund--refund will be done only if the trx is done in last 48 hrs.\n",
    "# the details of refund is stored in refund table,wehave ticket id,user name,transaction id,transaction amount,ticket raise time\n",
    "# the company automatically do refunds in 30mis, so we need to validate the request before initiating refund.\n",
    "# so for that we will create a new table called valid refund table which has-ticket id, trx id,user id,price,ticket raise time\n",
    "\n",
    "# so we have 1 sql file and 4 mysql tables apart fromthat we have 1 more table called accounts table which has details of\n",
    "# account number of customers-- but for the scope of this project lets suppose we dont have access tothat.\n",
    "\n",
    "# understanding requirements of etl pipeline---\n",
    "# signup summary gets updated every 5 mins , and we need data added in the users table in the last 5 mins to generate the results\n",
    "# this is the first pipeline that we need to build.\n",
    "# another requirement is we need to generate a trx summary table-in this table we will store the no.of trxs product category wise \n",
    "# and product brand wise. It will also store the total sales brand wise and product wise.This table gets updated every 10 mins\n",
    "# to calc this data we will require trx tableand product data csv file.This is the second pipeline\n",
    "# another requirement is we need to update valid refund table-for that we require data from 3 different sources-\n",
    "# 1 refund table, we will collect last request every 30 mins, trxs- trxids generated in last 48 hours, from valid refunds\n",
    "# we will check if the data is refund or not in a last 48hrs.\n",
    "# so we have 2 types of reuests valid and invalid,we will put the result in csv file and we will assign the reason as well\n",
    "\n",
    "# invalid request csv has ticket id,user name,refund reject reason\n",
    "# apart fromthat we need to update the valid trx table for which the refund req is found correct\n",
    "\n",
    "# this is the 3rd etl pipeline, and it runs every 30 mins\n",
    "\n",
    "\n",
    "# now goto sql workbench---import etl_dump.sqlscript in workbench and create allthe tables--etl_dump.sql file\n",
    "\n",
    "# then see 1.Generate User Data.ipynb-- u donot need to understand this file fully\n",
    "\n",
    "\n",
    "# keep on running 1.Generate User Data.ipynb in the mean time goto workbench\n",
    "\n",
    "#select count(*) from users; 246 rows have been entered.\n",
    "\n",
    "# because generate user data.ipynb is running, rows will be kept on adding in users.u\n",
    "\n",
    "#then goto 2.generate transactions data\n",
    "\n",
    "# in product_table.csv we have product_id,product_name,product_categoryand product_price-- we are not updating anything\n",
    "\n",
    "# in  this file. Then we have 3. generate refund table-\n",
    "\n",
    "# now,above we have setup a simulation environment to generate realtime data.Now,before going into etl pipeline--we\n",
    "# will first undrstand our pipeline--\n",
    "# for extraction we have extract_users_data() will extract data oflast 5 mins fromusers table.-- this func will pass the \n",
    "# data to transform_users_data() wich will do required transformation .Finally,load_user_summary() for loading data into\n",
    "# target table.All these steps will be executed in order every 5mins.\n",
    "# for the 2nd pipeline ,we need to extract product data from csv file fo which define extract_product_data().. we also\n",
    "# need to extract trx data in extract_transaction_data()... data from both these functions are passed into transform_trx_data()\n",
    "# and finally load_transaction_summary() will load the data into required target table--- all these steps will be executed \n",
    "# every 10 mins\n",
    "# For the finalpipeline, we need to extract_transaction_data(),extract_refund_data(),extract_valid_refund_data()-- we\n",
    "# will use all these result and pass it into transform_refund_data() for required trnsformation and finally load_refund_table()\n",
    "# togenerate invalid request csv file and also upload vald data into refund table-- Nowthis pipeline is executed every 30 mins\n",
    "\n",
    "# for all these pipelines we will needa single DB connection\n",
    "\n",
    "# now datain extract_product_data() changes rarely and we donot need to extract it every 10 mins. So we will use this func\n",
    "# outside the pipelin\n",
    "\n",
    "# now we will start building etl pipeline --\n",
    "\n",
    "# etl pipeline 1 to update user summary table--\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/pipeline.png)\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "import schedule\n",
    "import time\n",
    "import pandas as pd\n",
    "import mysql.connector as mysql\n",
    "from datetime import timedelta\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the database\n",
    "db = mysql.connect(\n",
    "    host = \"localhost\",\n",
    "    user = \"root\",  ## Enter your user name here\n",
    "    #password = \"ABC@123\", ## Enter your password here\n",
    "    database = \"etl\", ## Enter the database name here\n",
    "    #auth_plugin = \"mysql_native_password\",\n",
    ")\n",
    "# create the cursor\n",
    "cursor = db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now for the entirety of this project keep the generate user data,trx data,refund data files running until we r finished \n",
    "# with all the pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining extraction function--extract_user_data() for extraction of data\n",
    "# parameters required--database connection string to connect and query database table, start time and end time to modify query\n",
    "# output - panas df of extracted data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "#### `EXTRACT`\n",
    "\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining transformation function--\n",
    "#transform_user_data() it will use data from extract_user_data() oflast 5 min and do following transformation using pandas\n",
    "# replace the category not available with organic in the source col,use group by func to calcno. of users in each category\n",
    "# of source, use group by func to calc no. of prime users in each category of source,store all the results in a dict\n",
    "# add the start and end time in the dictionary and return it.\n",
    "# parameter required- df_user - user data dataframe of last 5 mins,start time and end time toupdate the o/p,so that we know\n",
    "# the signup summary timeline\n",
    "# it will return dictionary that will be used to import data into users summary table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the extract function\n",
    "def extract(db):\n",
    "    \n",
    "    # create database cursor\n",
    "    cursor = db.cursor()\n",
    "    \n",
    "    # current time & time 5 minutes before\n",
    "    now = datetime.datetime.now()\n",
    "    now_minus_5 = now - datetime.timedelta(minutes = 10)\n",
    "    \n",
    "    print(\"Extracting results between {} and {}\".format(str(now_minus_5),str(now)))\n",
    "    \n",
    "    # command to extract the data of last 5 minutes.\n",
    "    command = \"SELECT * FROM users WHERE signup_time BETWEEN '{}' AND '{}'\".format(now_minus_5, now)\n",
    "    \n",
    "    # execute the command and return the results.\n",
    "    cursor.execute(command)\n",
    "    data = cursor.fetchall()\n",
    "    return now_minus_5, now, pd.DataFrame.from_records(data, columns= ['user_id',\n",
    "                                                     'user_email',\n",
    "                                                     'user_name',\n",
    "                                                     'source',\n",
    "                                                     'is_prime',\n",
    "                                                     'signup_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Transform`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the tranform function\n",
    "def transform(df, start_time, end_time):\n",
    "    print(\"Transformation\")\n",
    "    df.source.replace(\"Not Available\", \"Organic\", inplace=True)\n",
    "    \n",
    "    source_count = df.groupby(['source'])['user_id'].count()\n",
    "    prime_count = df.groupby(['source'])['is_prime'].sum()\n",
    "    \n",
    "    source_count_dict = source_count.to_dict()\n",
    "    for key in prime_count.to_dict():\n",
    "        new_key_name = \"prime_from_\" + key\n",
    "        source_count_dict[new_key_name] = prime_count[key]\n",
    "    \n",
    "    source_count_dict['start_time'] = str(start_time)\n",
    "    source_count_dict['end_time'] = str(end_time)\n",
    "    \n",
    "    \n",
    "    return source_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining loading func- load_users_summary()-it will use the results dict from transforms_user_data and load itinto\n",
    "# signup_summary table\n",
    "# parameters required-result_dict-finalresults dictionary from the transform func,dbconn toupdate values in table\n",
    "# it will o/p or return nothing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `LOAD`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(source_count_dict, db):\n",
    "    print(\"Loading\")\n",
    "    cursor = db.cursor()\n",
    "    command = \"INSERT INTO signup_summary({col}) values{val}\".format(col= ','.join(source_count_dict.keys()),\n",
    "                                                                       val= tuple(source_count_dict.values()))\n",
    "    cursor.execute(command)\n",
    "    db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining etl pipeline-take db conn as parameter and do foll- define timefor which u want to extract the data,\n",
    "# extract latest 5 mins of users data, transform it to get users signup summary,load the data into sign_up summary table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Define the ETL Pipeline\n",
    "\n",
    "- Step 1: Extraction\n",
    "- Step 2: Transformation\n",
    "- Step 3: Loading   \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job():\n",
    "\n",
    "    db_object = create_connection()\n",
    "    \n",
    "    ## STEP 1: Extract\n",
    "    start_time, end_time, df = extract(db_object)\n",
    "    \n",
    "    ## Step 2: Transform\n",
    "    final_results = transform(df, start_time, end_time)\n",
    "    \n",
    "    ## Step 3: Load\n",
    "    load(final_results, db_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Every 300 seconds do job() (last run: [never], next run: 2023-01-18 09:31:58)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schedule.every(300).seconds.do(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      2\u001b[0m         schedule\u001b[38;5;241m.\u001b[39mrun_pending() \n\u001b[1;32m----> 3\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "        schedule.run_pending() \n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "        job()\n",
    "        time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=\"mylogfile.log\",  level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    logging.info(msg= \"You have a new log entry!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
