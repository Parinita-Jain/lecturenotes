{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "<center><h1> Project: ETL Pipeline </h1></center>\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Importing the required libraries`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "import schedule\n",
    "import time\n",
    "import pandas as pd\n",
    "import mysql.connector as mysql\n",
    "from datetime import timedelta\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "***Define a function to create the connection with the MySQL database. `Configure the following cell as per your system settings.`***\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the database\n",
    "db = mysql.connect(\n",
    "    host = \"localhost\",\n",
    "    user = \"root\",  ## Enter your user name here\n",
    "    #password = \"ABC@123\", ## Enter your password here\n",
    "    database = \"etl\", ## Enter the database name here\n",
    "    #auth_plugin = \"mysql_native_password\",\n",
    ")\n",
    "# create the cursor\n",
    "cursor = db.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "<center><h1> ETL Pipeline 1 </h1></center>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "![](images/pipeline-1.png)\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### `Define the Extraction Function`\n",
    "\n",
    "---\n",
    "\n",
    "* ***`extract_users_data()`***: It will extract the data from the `users` table within defined time range and return the dataframe.\n",
    "\n",
    "<br>\n",
    "\n",
    "* ***`Paramaters Required`***:\n",
    "    * `Database Connection String` is required so that it can connect to the database and query the data. \n",
    "    * `Start time` and `End time` is required so that we can modify the query.\n",
    "\n",
    "<br>\n",
    "\n",
    "* ***`Output`***: It will return the pandas dataframe of the extracted data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_users_data(db, start_time, end_time):\n",
    "    \n",
    "    # create database cursor\n",
    "    cursor = db.cursor()\n",
    "    \n",
    "    print(\"Extracting signup results between {} and {}\".format(str(start_time),str(end_time)))\n",
    "    \n",
    "    # command to extract the data of last 5 minutes.\n",
    "    command = \"SELECT * FROM users WHERE signup_time BETWEEN '{}' AND '{}'\".format(start_time, end_time)\n",
    "    \n",
    "    # execute the command and return the results.\n",
    "    cursor.execute(command)\n",
    "    data = cursor.fetchall()\n",
    "    \n",
    "    # return the dataframe\n",
    "    return pd.DataFrame.from_records(data, columns= ['user_id',\n",
    "                                                     'user_email',\n",
    "                                                     'user_name',\n",
    "                                                     'source',\n",
    "                                                     'is_prime',\n",
    "                                                     'signup_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Define the Trasformation Function`\n",
    "\n",
    "---\n",
    "\n",
    "* ***`transform_users_data()`***: It will use the data from the `extract_users_data()` of the last 5 minutes and do the following transformation using pandas.\n",
    "    * Replace the category `Not Available` with the `Organic` in the source column.\n",
    "    * Use the groupby function to calculate number of users in each category of source.\n",
    "    * Use the groupby function to calculate number of prime users in each category of source.\n",
    "    * Store all the results in a dictionary \n",
    "    * Add the start & end time in the dictionary and return it.\n",
    "    \n",
    "<br>    \n",
    "\n",
    "* ***`Paramaters Required`***:\n",
    "    * `df_user` user data dataframe of last 5 minutes. \n",
    "    * `Start time` and `End time` is required to update the output, so that we know the signup summary is between this particulae time range.\n",
    "\n",
    "<br>\n",
    "\n",
    "* ***`Output`***: It will return the dictionary that will be used to import the data into the transaction summary table.\n",
    "\n",
    "---\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the tranform function\n",
    "def transform_user_data(df_user, start_time, end_time):\n",
    "    \n",
    "    print(\"Transforming User Data...\")\n",
    "    \n",
    "    # replace the \"Not Available\" with the \"Organic\"\n",
    "    df_user.source.replace(\"Not Available\", \"Organic\", inplace=True)\n",
    "    \n",
    "    # groupby to calculate the number of users in each category of source.\n",
    "    source_count = df_user.groupby(['source'])['user_id'].count()\n",
    "    \n",
    "    # groupby to calculate the number of prime users in each category of source.\n",
    "    prime_count  = df_user.groupby(['source'])['is_prime'].sum()\n",
    "    \n",
    "    # create dictionary of source count\n",
    "    source_count_dict = source_count.to_dict()\n",
    "    \n",
    "    # append prime count in the same dictionary\n",
    "    for key in prime_count.to_dict():\n",
    "        new_key_name = \"prime_from_\" + key\n",
    "        source_count_dict[new_key_name] = prime_count[key]\n",
    "    \n",
    "    # add start_time and end_time to the dictionary \n",
    "    source_count_dict['start_time'] = str(start_time)\n",
    "    source_count_dict['end_time'] = str(end_time)\n",
    "    \n",
    "    # return the final dictionary\n",
    "    return source_count_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Define the Loading Function`\n",
    "\n",
    "---\n",
    "\n",
    "* ***`load_users_summary()`***: It will use the results dictionary from the `transform_users_data` and load it into the `signup_summary_table`.\n",
    "\n",
    "    \n",
    "<br>    \n",
    "\n",
    "* ***`Paramaters Required`***:\n",
    "\n",
    "    * `result_dict` final results dictionary from the transform function. \n",
    "    * `db` database connection string to update the values in the table.\n",
    "\n",
    "<br>\n",
    "\n",
    "* ***`Output`***: It will not return anything.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_user_summary(result_dict, db):\n",
    "    print(\"Loading User Summary Table...\")\n",
    "    cursor = db.cursor()\n",
    "    \n",
    "    # command to insert the data into the signup summary table using result dict\n",
    "    command = \"INSERT INTO signup_summary({col}) values{val}\".format(col= ','.join(result_dict.keys()),\n",
    "                                                                     val= tuple(result_dict.values()))\n",
    "    cursor.execute(command)\n",
    "    db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "#### `Define the ETL Pipeline`\n",
    "\n",
    "<br>\n",
    "\n",
    "Now, we will define the pipeline function, we will take the database connection as the parameter and do the following steps.\n",
    "\n",
    " * Now, we will define the time for which we want to extract the data.\n",
    " * Extract the latest 5 minutes of users data.\n",
    " * Transform it to get the users signup summary.\n",
    " * Load the data into the signup_summary table.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def pipeline_to_update_user_summary(db_object):\n",
    "    \n",
    "    # get the current time and time before 5 minutes\n",
    "    current_time = datetime.datetime.now()\n",
    "    current_minus_5 = current_time - datetime.timedelta(minutes=5)\n",
    "    \n",
    "    print(\"========================================================================================\")\n",
    "    print(\"Starting ETL to update user summary!!\")\n",
    "    \n",
    "    ## EXTRACTION\n",
    "    latest_user_data = extract_users_data(db = db_object,\n",
    "                                          start_time = current_minus_5,\n",
    "                                          end_time=current_time)\n",
    "    \n",
    "    ## TRANSFORMATION\n",
    "    user_summary_data = transform_user_data(df_user= latest_user_data,\n",
    "                                            start_time=current_minus_5, \n",
    "                                            end_time=current_time)\n",
    "    \n",
    "    ## LOADING\n",
    "    load_user_summary(result_dict = user_summary_data,\n",
    "                      db = db_object)\n",
    "    \n",
    "    print(\"Successfully loaded the data into user summary table !!\")\n",
    "    print(\"========================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "<center><h1> ETL Pipeline 2 </h1></center>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "![](images/pipeline-2.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Define the extraction functions`\n",
    "\n",
    "---\n",
    "\n",
    "* ***`extract_products_data()`***: It will read the products data from the CSV file. \n",
    "\n",
    "* ***`Paramaters Required`***: CSV file path is required. It is present in the `dataset` folder.\n",
    "\n",
    "* ***`Output`***: It will return the pandas dataframe of the CSV file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to extract the product_data from the CSV file\n",
    "def extract_products_data(file_path):\n",
    "    return pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "* ***`extract_transaction_data()`***: It will extract the data from the `transaction` table within a defined time range and return the dataframe.\n",
    "\n",
    "* ***`Paramaters Required`***:\n",
    "    * `Database Connection String` is required so that it can connect to the database and query the data. \n",
    "    * `Start time` and `End time` is required so that we can modify the query.\n",
    "\n",
    "* ***`Output`***: It will return the pandas dataframe of the extracted data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_transaction_data(db, start_time, end_time):\n",
    "    \n",
    "    # create database cursor\n",
    "    cursor = db.cursor()\n",
    "    \n",
    "    print(\"Extracting transactions between {} and {}\".format(str(start_time),str(end_time)))\n",
    "    \n",
    "    # command to extract the data of last 5 minutes.\n",
    "    command = \"SELECT * FROM transaction WHERE transaction_time BETWEEN '{}' AND '{}'\".format(start_time, end_time)\n",
    "    \n",
    "    # execute the command and return the results.\n",
    "    cursor.execute(command)\n",
    "    data = cursor.fetchall()\n",
    "    \n",
    "    # return the dataframe\n",
    "    return pd.DataFrame.from_records(data, columns= ['transaction_id',\n",
    "                                                     'user_id',\n",
    "                                                     'product_id',\n",
    "                                                     'transaction_time',\n",
    "                                                     'price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Define the Trasformation Function`\n",
    "\n",
    "---\n",
    "\n",
    "* ***`transform_transaction_data()`***: It will use the data from the `extract_transform_data()` of the last 10 minutes and the output of `extract_products_data()` and do the following transformation using pandas.\n",
    "    * Do the left join on `transction_data` and the `products_data`.\n",
    "    * Split the product_name and create a new feature `brand`.\n",
    "    * Use groupby to calculate the brand-wise sales.\n",
    "    * Use groupby to calculate the category-wise sales. \n",
    "    * Create dictionary of the calculated results.\n",
    "    * Add start and end time to the dictionary.\n",
    "    \n",
    "<br>    \n",
    "\n",
    "* ***`Paramaters Required`***:\n",
    "    * `df_transaction` transaction data dataframe of last 10 minutes. \n",
    "    * `df_product` product data frame.\n",
    "    * `Start time` and `End time` is required to update the output, so that we know the signup summary is between this particulae time range.\n",
    "\n",
    "<br>\n",
    "\n",
    "* ***`Output`***: It will return the dictionary which will be used to import the data into transaction summary.\n",
    "\n",
    "---\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the tranform function\n",
    "def transform_transaction_data(df_transaction, df_product, start_time, end_time):\n",
    "    \n",
    "    print(\"Transforming Transaction Data...\")\n",
    "    \n",
    "    # merge the transaction and product dataframe.\n",
    "    merged_df = df_transaction.merge(df_product, how='left', on='product_id')\n",
    "    \n",
    "    # split the product_name to get the brand  \n",
    "    merged_df['brand'] = merged_df['product_name'].apply(lambda x: x.split()[0])\n",
    "    \n",
    "    # calculate the brand count\n",
    "    brand_count = merged_df.groupby(['brand'])['transaction_id'].count()\n",
    "    \n",
    "    # calculate the category count\n",
    "    category_count = merged_df.groupby(['product_category'])['transaction_id'].count()\n",
    "    \n",
    "    # calculate the brand wise sales\n",
    "    brand_wise_sales    = merged_df.groupby(['brand'])['price'].sum()\n",
    "    \n",
    "    # calculate the category wise sales\n",
    "    category_wise_sales = merged_df.groupby(['product_category'])['price'].sum()\n",
    "    \n",
    "    # create dictionary \n",
    "    brand_count_dict = brand_count.to_dict()\n",
    "    \n",
    "    # append brand_count to dictionary\n",
    "    for key in category_count.to_dict():\n",
    "        brand_count_dict[key] = category_count[key]\n",
    "    \n",
    "    # append brand wise sales to dictionary\n",
    "    for key in brand_wise_sales.to_dict():\n",
    "        new_key_name = \"sales_from_\" + key\n",
    "        brand_count_dict[new_key_name] = brand_wise_sales[key]\n",
    "    \n",
    "    # append category wise sales to dictionary\n",
    "    for key in category_wise_sales.to_dict():\n",
    "        new_key_name = \"sales_from_\" + key\n",
    "        brand_count_dict[new_key_name] = category_wise_sales[key]\n",
    "    \n",
    "    # append start and end time to the dictionary\n",
    "    brand_count_dict['start_time'] = str(start_time)\n",
    "    brand_count_dict['end_time'] = str(end_time)\n",
    "    \n",
    "    return brand_count_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Define the Loading Function`\n",
    "\n",
    "---\n",
    "\n",
    "* ***`load_transaction_summary()`***: It will use the results dictionary from the `transform_transaction_data` and load it into the `transaction_summary_table`.\n",
    "\n",
    "    \n",
    "<br>    \n",
    "\n",
    "* ***`Paramaters Required`***:\n",
    "    * `result_dict` final results dictionary from the transform function. \n",
    "    * `db` database connection string to update the values in the table.\n",
    "\n",
    "<br>\n",
    "\n",
    "* ***`Output`***: It will not return anything.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transaction_summary(result_dict, db):\n",
    "    print(\"Loading Transaction Summary Table...\")\n",
    "    cursor = db.cursor()\n",
    "    \n",
    "    # command to insert the data into the transaction summary using the \n",
    "    command = \"INSERT INTO transaction_summary({col}) values{val}\".format(col= ', '.join(result_dict.keys()),\n",
    "                                                                       val= tuple(result_dict.values()))\n",
    "    \n",
    "    \n",
    "\n",
    "    command = command.replace(' Air Conditioner', '`Air Conditioner`')\n",
    "    command = command.replace('sales_from_Air Conditioner', '`sales_from_Air Conditioner`')\n",
    "    \n",
    "    cursor.execute(command)\n",
    "    db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_to_update_transaction_summary(db_object, products_data):\n",
    "    \n",
    "    current_time = datetime.datetime.now()\n",
    "    current_minus_10 = current_time - datetime.timedelta(minutes=10)\n",
    "    \n",
    "    print(\"========================================================================================\")\n",
    "    print(\"Starting ETL to update transaction summary!!\")\n",
    "    \n",
    "    ## EXTRACTION\n",
    "    latest_transaction_data = extract_transaction_data(db = db_object,\n",
    "                                                       start_time = current_minus_10,\n",
    "                                                       end_time = current_time)\n",
    "    \n",
    "    ## TRANSFORMATION\n",
    "    transaction_summary_data = transform_transaction_data(df_product = products_data,\n",
    "                                                          df_transaction = latest_transaction_data,\n",
    "                                                          start_time = current_minus_10,\n",
    "                                                          end_time = current_time)\n",
    "    ## LOADING\n",
    "    load_transaction_summary(result_dict = transaction_summary_data,\n",
    "                             db = db_object)\n",
    "    \n",
    "    print(\"Successfully loaded the data into transaction summary table !!\")\n",
    "    print(\"========================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "\n",
    "<center><h1> ETL Pipeline 3 </h1></center>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "![](images/pipeline-3.png)\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "* ***`extract_refund_data()`***: It will extract the data from the `refund_detail` table within the defined time range and return the dataframe.\n",
    "\n",
    "* ***`Paramaters Required`***:\n",
    "    * `Database Connection String` is required so that it can connect to the database and query the data. \n",
    "    * `Start time` and `End time` is required so that we can modify the query.\n",
    "\n",
    "* ***`Output`***: It will return the pandas dataframe of the CSV file.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_refund_data(db, start_time, end_time):\n",
    "    \n",
    "    # create database cursor\n",
    "    cursor = db.cursor()\n",
    "    \n",
    "    print(\"Extracting refund between {} and {}\".format(str(start_time),str(end_time)))\n",
    "    \n",
    "    # command to extract the data of last 30 minutes.\n",
    "    command = \"SELECT * FROM refund_detail WHERE ticket_raise_time BETWEEN '{}' AND '{}'\".format(start_time, end_time)\n",
    "    \n",
    "    # execute the command and return the results.\n",
    "    cursor.execute(command)\n",
    "    data = cursor.fetchall()\n",
    "    \n",
    "    # return the dataframe\n",
    "    return pd.DataFrame.from_records(data, columns= ['ticket_id',\n",
    "                                                     'user_name',\n",
    "                                                     'transaction_id',\n",
    "                                                     'transaction_amount',\n",
    "                                                     'ticket_raise_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "* ***`extract_valid_refund_data()`***: It will extract only the `transaction_id` from the `valid_refund` table within defined time range and return the dataframe.\n",
    "\n",
    "* ***`Paramaters Required`***:\n",
    "    * `Database Connection String` is required so that it can connect to the database and query the data. \n",
    "    * `Start time` and `End time` is required so that we can modify the query.\n",
    "\n",
    "* ***`Output`***: It will return the pandas dataframe of the CSV file.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_valid_refund_data(db, start_time, end_time):\n",
    "    \n",
    "    # create database cursor\n",
    "    cursor = db.cursor()\n",
    "    \n",
    "    print(\"Extracting valid refund data between {} and {}\".format(str(start_time),str(end_time)))\n",
    "    \n",
    "    # command to extract the data of last 30 minutes.\n",
    "    command = \"SELECT transaction_id FROM valid_refund WHERE ticket_raise_time BETWEEN '{}' AND '{}'\".format(start_time, end_time)\n",
    "    \n",
    "    # execute the command and return the results.\n",
    "    cursor.execute(command)\n",
    "    data = cursor.fetchall()\n",
    "    \n",
    "    # return the valid transaction IDs list\n",
    "    return pd.DataFrame.from_records(data, columns= ['ticket_raise_time'])['ticket_raise_time'].to_list()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Define the Trasformation Function`\n",
    "\n",
    "---\n",
    "\n",
    "* ***`transform_users_data()`***: It will use the data from the `extract_users_data()` of the last 5 minutes and do the following transformation using pandas.\n",
    "    * Replace the category `Not Available` with the `Organic` in the source column.\n",
    "    * Use the groupby function to calculate number of users in each category of source.\n",
    "    * Use the groupby function to calculate number of prime users in each category of source.\n",
    "    * Store all the results in a dictionary \n",
    "    * Add the start & end time in the dictionary and return it.\n",
    "    \n",
    "<br>    \n",
    "\n",
    "* ***`Paramaters Required`***:\n",
    "    * `df_user` user data dataframe of last 5 minutes. \n",
    "    * `Start time` and `End time` is required to update the output, so that we know the signup summary is between this particulae time range.\n",
    "\n",
    "<br>\n",
    "\n",
    "* ***`Output`***: It will return the dictionary with the user summary data.\n",
    "\n",
    "---\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_refund_data(df_refund, df_transactions, refund_issued_TID):\n",
    "    \n",
    "    print(\"Transforming Refund Data...\")\n",
    "    \n",
    "    valid_transactions = df_transactions.merge(df_refund, how='left', on='transaction_id')\n",
    "    valid_transactions = valid_transactions[valid_transactions.ticket_id.isnull() == False]\n",
    "    \n",
    "    \n",
    "    valid_transactions = valid_transactions.groupby(['transaction_id']).first().reset_index()\n",
    "    \n",
    "    valid_transactions_final = valid_transactions[~valid_transactions.transaction_id.isin(refund_issued_TID)]\n",
    "    valid_transaction_duplicate = valid_transactions[valid_transactions.transaction_id.isin(refund_issued_TID)]\n",
    "    \n",
    "    \n",
    "    \n",
    "    valid_transactions_final = valid_transactions_final[['ticket_id', 'transaction_id', 'user_id', 'price', 'ticket_raise_time']]\n",
    "    \n",
    "    valid_transactions_final = [tuple((row[0],row[1],row[2],int(row[3]), str(row[4]))) for row in valid_transactions_final.to_records(index=False)]\n",
    "    \n",
    "    \n",
    "    refund_rejected = df_refund.merge(df_transactions, how='left', on='transaction_id')\n",
    "    refund_rejected['refund_reject_reason'] = None\n",
    "    refund_rejected.loc[refund_rejected.product_id.isna() == True, 'refund_reject_reason'] = 'transaction_id_not_matched'\n",
    "    refund_rejected.loc[refund_rejected.transaction_id.isin(refund_issued_TID), 'refund_reject_reason'] = 'already_processed'\n",
    "    \n",
    "    \n",
    "    refund_rejected = refund_rejected[refund_rejected.refund_reject_reason.isna() == False]\n",
    "    refund_rejected = refund_rejected[['ticket_id', 'user_name', 'refund_reject_reason']]\n",
    "    \n",
    "    \n",
    "    return valid_transactions_final, refund_rejected\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Define the Loading Function`\n",
    "\n",
    "---\n",
    "\n",
    "* ***`load_users_summary()`***: It will use the results dictionary from the `transform_users_data` and load it into the `signup_summary_table`.\n",
    "\n",
    "    \n",
    "<br>    \n",
    "\n",
    "* ***`Paramaters Required`***:\n",
    "    * `result_dict` final results dictionary from the transform function. \n",
    "    * `db` database connection string to update the values in the table.\n",
    "\n",
    "<br>\n",
    "\n",
    "* ***`Output`***: It will not return anything.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_refund_data(valid_refunds, rejected_requests, db):\n",
    "    print(\"Loading Valid Refunds Table...\")\n",
    "    cursor = db.cursor()\n",
    "    \n",
    "    command = \"INSERT INTO valid_refund(ticket_id, transaction_id, user_id, price, ticket_raise_time) values(%s, %s, %s, %s, %s)\"\n",
    "    cursor.executemany(command, valid_refunds)\n",
    "    db.commit()\n",
    "    \n",
    "    file_name = \"rejected_request\" + str(pd.datetime.now()) + \".csv\"\n",
    "    rejected_requests.to_csv(\"output_folder/\" + file_name)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_to_update_valid_refunds(db_object):\n",
    "    \n",
    "    current_time = datetime.datetime.now()\n",
    "    current_minus_30 = current_time - datetime.timedelta(minutes = 30)\n",
    "    current_minus_48 = current_time - datetime.timedelta(hours  = 48)\n",
    "    \n",
    "    print(\"========================================================================================\")\n",
    "    print(\"Starting ETL to update valid refunds data!!\")\n",
    "    \n",
    "    ## EXTRACTION\n",
    "    latest_refund_requests = extract_refund_data(db = db_object,\n",
    "                                                 start_time = current_minus_30,\n",
    "                                                 end_time = current_time)\n",
    "    \n",
    "    valid_transactions = extract_transaction_data(db = db_object,\n",
    "                                                  start_time = current_minus_48,\n",
    "                                                  end_time = current_time)\n",
    "   \n",
    "    refund_issued = extract_valid_refund_data(db = db_object,\n",
    "                                              start_time = current_minus_48,\n",
    "                                              end_time = current_time)\n",
    "    \n",
    "    ## TRANSFORMATION\n",
    "    valid_refunds, refunds_rejected = transform_refund_data(df_refund=latest_refund_requests,\n",
    "                                                            df_transactions=valid_transactions,\n",
    "                                                            refund_issued_TID= refund_issued)\n",
    "    \n",
    "    ## LOADING\n",
    "    load_refund_data(db = db_object,\n",
    "                     valid_refunds = valid_refunds,\n",
    "                     rejected_requests = refunds_rejected)\n",
    "    \n",
    "    print(\"Successfully loaded the data into valid refund table !!\")\n",
    "    print(\"========================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<center><h1> Schedule Pipelines </h1></center>\n",
    "\n",
    "---\n",
    "\n",
    "![](images/pipeline-4.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Every 150 seconds do pipeline_to_update_valid_refunds(db_object=<mysql.connector.connection_cext.CMySQLConnection object at 0x7f3fc4c52b50>) (last run: [never], next run: 2021-09-15 16:43:56)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Schedule Pipelines\n",
    "\n",
    "db_object = create_connection()\n",
    "products_data = pd.read_csv('dataset/product_table.csv')\n",
    "\n",
    "schedule.every(30).seconds.do(pipeline_to_update_user_summary, db_object = db_object)\n",
    "schedule.every(60).seconds.do(pipeline_to_update_transaction_summary, db_object = db_object, products_data = products_data)\n",
    "schedule.every(150).seconds.do(pipeline_to_update_valid_refunds, db_object = db_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================\n",
      "Starting ETL to update user summary!!\n",
      "Extracting signup results between 2021-09-15 16:36:56.588452 and 2021-09-15 16:41:56.588452\n",
      "Transforming User Data...\n",
      "Loading User Summary Table...\n",
      "Successfully loaded the data into user summary table !!\n",
      "========================================================================================\n",
      "========================================================================================\n",
      "Starting ETL to update transaction summary!!\n",
      "Extracting transactions between 2021-09-15 16:32:26.952669 and 2021-09-15 16:42:26.952669\n",
      "Transforming Transaction Data...\n",
      "Loading Transaction Summary Table...\n",
      "Successfully loaded the data into transaction summary table !!\n",
      "========================================================================================\n",
      "========================================================================================\n",
      "Starting ETL to update user summary!!\n",
      "Extracting signup results between 2021-09-15 16:37:27.689811 and 2021-09-15 16:42:27.689811\n",
      "Transforming User Data...\n",
      "Loading User Summary Table...\n",
      "Successfully loaded the data into user summary table !!\n",
      "========================================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-3947d0ce50b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m         \u001b[0mschedule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pending\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "        schedule.run_pending() \n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create table valid_refund(ticket_id varchar(50), transaction_id varchar(50), user_id varchar(50), price int);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
